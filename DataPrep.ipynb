{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **1st** Notebook in the clustering pipeline. It allows you to take the semi-raw riskfactor data (prepped by Patricia), as well as the raw biomarker data, and prep it for future analysis, by putting it into a DataSet object.\n",
    "\n",
    "Use <u>***pappas_tadam***</u> virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to whatever directory GoodCopy is in, make sure to add a / at the end.\n",
    "\n",
    "home_dir = \"/home/l/lungboy/tadam/Project/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(home_dir + 'GoodCopy/Functions')\n",
    "\n",
    "import FunctionsOOPGood as func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Risk Factor Data\n",
    "\n",
    "This data was pre-prepped by Patricia to remove missingness, impute NAs and scale data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data prepped by Patricia\n",
    "\n",
    "data = pd.read_csv(home_dir + \"GoodCopy/Data/risk_factors.cleaned.scaled.one_hot.csv\")\n",
    "data_unscaled = pd.read_csv(home_dir + \"GoodCopy/Data/risk_factors.cleaned.unscaled.one_hot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unscaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomarker Data\n",
    "\n",
    "I prepped this data in the following way"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Biomarker Data\n",
    "\n",
    "bio_data1 = pd.read_excel(home_dir + \"GoodCopy/Data/BiomarkerRaw/AlereData_readable.xlsx\",header = 5)\n",
    "bio_data2 = pd.read_csv(home_dir + \"GoodCopy/Data/BiomarkerRaw/AlereData_2.csv\",header= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for matching columns and drop any that are the same\n",
    "\n",
    "drop_list = []\n",
    "\n",
    "for col in bio_data1.columns.tolist():\n",
    "    if col in bio_data2.columns.tolist():\n",
    "        drop_list.append(col)\n",
    "        \n",
    "# Drop duplicated columns and concatenate\n",
    "        \n",
    "bio_data2.drop(drop_list,axis = 1,inplace = True)\n",
    "bio_data = pd.concat([bio_data1,bio_data2],axis=1,verify_integrity=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "The following process is a bit messy, as we found overlapping patient IDs in the biomarker data, as some patient IDs had two different patients at two different sites. Thus we had to crossreference IDs with site to select the correct patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ids of patients we use for risk factor data. Need to use this risk data file as \n",
    "# it contains REGIDS\n",
    "\n",
    "raw_data_removed = pd.read_csv(home_dir + \"GoodCopy/Data/risk_factors.csv\", encoding= 'unicode_escape',index_col = \"regid\")\n",
    "raw_data_removed.drop(labels=\"Unnamed: 0\",inplace=True, axis = 1)\n",
    "ids = np.asarray(raw_data_removed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting biomarker list down to those patients only\n",
    "\n",
    "bio_data_removed = bio_data.copy()\n",
    "\n",
    "remove_list= [] # list of rows to remove\n",
    "check_list = [] # List of IDs that were checked already and kept\n",
    "\n",
    "for i in range(len(bio_data[\"SCOPE Subject ID\"].tolist())):\n",
    "    ID = bio_data[\"SCOPE Subject ID\"].tolist()[i] # patient ID number\n",
    "    \n",
    "    if ID not in ids or ID in check_list:\n",
    "        remove_list.append(i) # if the ID isnt in the list of current IDs or it has already been checked, \n",
    "                              # remove it\n",
    "    \n",
    "    elif ID not in check_list:\n",
    "        # check for the correct site\n",
    "        \n",
    "        if raw_data_removed.loc[ID, \"centre\"] != bio_data['Site'].tolist()[i]:\n",
    "            if bio_data['Site'].tolist()[i] == \"Adelaide Research & Innovation\" and raw_data_removed.loc[ID, \"centre\"] == \"Adelaide University\":\n",
    "                check_list.append(ID)\n",
    "                pass\n",
    "            \n",
    "                \n",
    "            elif bio_data['Site'].tolist()[i] == \"University of Manchester\" and raw_data_removed.loc[ID, \"centre\"] == \"Manchester University\":\n",
    "                check_list.append(ID)\n",
    "                pass\n",
    "                \n",
    "            elif bio_data['Site'].tolist()[i] == \"Kings College London\" and raw_data_removed.loc[ID, \"centre\"] == \"Kings College, London\":\n",
    "                check_list.append(ID)\n",
    "                pass\n",
    "            \n",
    "            elif bio_data['Site'].tolist()[i] == \"Cork University\" and raw_data_removed.loc[ID, \"centre\"] == \"University College, Cork\":\n",
    "                check_list.append(ID)\n",
    "                pass\n",
    "            \n",
    "            elif bio_data['Site'].tolist()[i] == \"Leeds (St. James)\" and raw_data_removed.loc[ID, \"centre\"] == \"University of Leeds\":\n",
    "                check_list.append(ID)\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                remove_list.append(i)\n",
    "        else:\n",
    "            check_list.append(ID)\n",
    "\n",
    "\n",
    "print(len(remove_list))\n",
    "                \n",
    "bio_data_removed.drop(index = remove_list,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop columns with more than 3000 NAs\n",
    "\n",
    "for col in bio_data_removed.columns:\n",
    "    if bio_data_removed[col].isna().sum() > 3000:\n",
    "        bio_data_removed.drop(columns = col,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scale\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler((1,2))\n",
    "# transform data\n",
    "scaled_bio_data = scaler.fit_transform(bio_data_removed.drop(columns = bio_data_removed.columns[[0,1,2,3,4,5]]))\n",
    "scaled_bio_data = pd.DataFrame(scaled_bio_data,index = bio_data_removed.index,columns = bio_data_removed.columns.tolist()[6:])\n",
    "scaled_bio_data.to_csv(home_dir + \"GoodCopy/Data/biomarker.rmNA.scaled.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "We chose to normalize data based on biomarkers that occur in similar rates among different patients. I.E. \"Adam 9 3b\" is ranked 3rd most frequent in 95% of patients, thus it is commonly ranked at the same frequency, as we can use to normalize patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ranked levels of protein expression for all patients\n",
    "# here all the rows are patients and the columns are the rank, ie for patient 0, Cystatin is \n",
    "# highest biomarker measurement.\n",
    "\n",
    "norm_list = []\n",
    "\n",
    "for i in scaled_bio_data.index:\n",
    "    norm_list.append(scaled_bio_data.loc[i].sort_values(ascending=False).index.tolist())\n",
    "    \n",
    "norm_df = pd.DataFrame(norm_list)\n",
    "display(norm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most commonly occuring protein in each rank, ie for 4th highest biomarker, the most frequent biomarker \n",
    "# among all patients is C-Met 111a.\n",
    "\n",
    "norm_df.mode(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank all variables as specified above and put into dataframe, where rank is the most common rank of the variable\n",
    "# and count is the number of times it appears at this rank. Thus, biomarkers with high counts commonly occur in\n",
    "# the same rank most of the time.\n",
    "\n",
    "temp = []\n",
    "ranked_vars = []\n",
    "\n",
    "\n",
    "for i in norm_df.columns:\n",
    "    temp.append(norm_df[i].value_counts().tolist()[0])\n",
    "\n",
    "for k in range(len(temp)):\n",
    "    m = 0\n",
    "    max_ind = 0\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] > m:\n",
    "            m = temp[i]\n",
    "            max_ind = i\n",
    "    ranked_vars.append([norm_df.mode()[max_ind][0],m,max_ind])\n",
    "    temp[max_ind] = np.NINF\n",
    "            \n",
    "ranked_vars_df = pd.DataFrame(ranked_vars, columns=[\"variable\",\"count\",\"rank\"])\n",
    "ranked_vars_df.sort_values(by=[\"count\"],ascending=False,inplace=True)\n",
    "ranked_vars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by top 3 most frequent variables from ranked_vars_df\n",
    "\n",
    "for pat in scaled_bio_data.index:\n",
    "    scaled_bio_data.loc[pat] = scaled_bio_data.loc[pat]/scaled_bio_data.loc[pat,ranked_vars_df.loc[0:2,\"variable\"]].sum()\n",
    "    \n",
    "scaled_bio_data.to_csv(home_dir + \"GoodCopy/Data/biomarker.rmNA.scaled.normalized_top3\")\n",
    "scaled_bio_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Site Labels\n",
    "\n",
    "prepping site labels for DataSet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting site labels from raw data\n",
    "\n",
    "site_labels = pd.read_csv(home_dir + \"GoodCopy/Data/risk_factors.csv\", encoding = \"cp1252\").centre\n",
    "site_labels.to_csv(home_dir + \"GoodCopy/Data/site_labels.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE Labels\n",
    "\n",
    "prepping PE labels for DataSet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting site labels from raw data\n",
    "\n",
    "pe_labels = pd.read_csv(home_dir + \"GoodCopy/Data/risk_factors.csv\", encoding = \"cp1252\").f34_pet\n",
    "pe_labels.to_csv(home_dir + \"GoodCopy/Data/pe_labels.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcome Variables\n",
    "\n",
    "The workflow I used to prep outcome variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a list of outcome variables. These were selected by starting\n",
    "# after week 24 and looking at which variables could be useful for\n",
    "# diferentiating severity and type of disease. More could be added if needed\n",
    "\n",
    "outcome_variables = ['f34_pet',\"f34c_gest_diag_pet\",\"f34c_mat_adm_gest_PET_PTB_SGA\",\n",
    "                    \"f34c_gest_dev_gh\",\"f35_Max_sBP_Adm\",\"f35_dBP_Adm\",\"f35_Max_dBP_Adm\",\n",
    "                     \"f35_sBP_Adm\", \"f35_Max_Pulse_Adm\", \"f35_Max_proturia_dipstick_Adm\",\n",
    "                    \"f35c_f24_any_proturia\", \"f37_Hb_Lowest_ap\",\n",
    "                    \"f37_Hb_Highest_ap\",\"f37_hct_Lowest_ap\",\"f37_hct_Highest_ap\",\n",
    "                    \"f37_wcc_lowest_ap\",\"f37_wcc_highest_ap\",\"f37_platelets_lowest_ap\",\n",
    "                    \"f37_platelets_highest_ap\",\"f37_prot_creat_lowest_ap\",\n",
    "                     \"f37_prot_creat_highest_ap\",\"f37_24hproturia_lowest_ap\",\n",
    "                    \"f37_24hproturia_highest_ap\",\"f37_creat_Lowest_ap_umol\",\n",
    "                    \"f37_creat_highest_ap_umol\",\"f37_urate_lowest_ap_mmol\",\n",
    "                     \"f37_urate_highest_ap_mmol\",\"f37_AST_lowest_ap\",\"f37_AST_highest_ap\",\n",
    "                    \"f37_ALT_lowest_ap\",\"f37_ALT_highest_ap\",\"f37_GGT_lowest_ap\",\n",
    "                     \"f37_GGT_highest_ap\",\"f37_billi_lowest_ap_umol\",\"f37_billi_highest_ap_umol\",\n",
    "                    \"f37_Alb_lowest_ap\",\"f37_Alb_highest_ap\",\"f37_LDH_lowest_ap\",\n",
    "                     \"f37_LDH_highest_ap\",\"f37_Haptoglobin_lowest_ap\",\"f37_APTT_highest_ap\",\n",
    "                    \"f37_PR_highest_ap\",\"f37_Ddimer_highest_ap\",\"f37_CRP_lowest_ap\",\n",
    "                    \"f39_fetal_outcome\", \"f39c_final_del_gest\",\"f39_status_after_2nd_vst\",\n",
    "                     \"f39_pet\",\"f39c_pet_lt_37w\",\"f39c_pet_ge_37w\",\"f39c_SGA_AGA_LGA\",\n",
    "                     \"f25_Placental_Wgt\",\"f26_Birthwgt\",\"f26_Length\",\"f38c_hellp\",\"f38c_ellp\"]\n",
    "\n",
    "outcome_variables.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data dictionary\n",
    "\n",
    "data_dict = pd.read_csv(home_dir + \"GoodCopy/Data/DataDict_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with outcome variables and their descriptions\n",
    "\n",
    "# Get explanation for each variable\n",
    "\n",
    "variable_explanation = []\n",
    "for e in outcome_variables:\n",
    "    variable_explanation.append(list(data_dict[\" Variable Explanation\\n\"])[list(data_dict[\"Variable SAS name\"]).index(e)])\n",
    "    \n",
    "# Get variable data types\n",
    "\n",
    "variable_type = []\n",
    "for e in outcome_variables:\n",
    "    variable_type.append(list(data_dict[\"Database Categories\"])[list(data_dict[\"Variable SAS name\"]).index(e)])\n",
    "  \n",
    "# Create dataframe\n",
    "\n",
    "outcomes = pd.DataFrame()\n",
    "outcomes[\"variable\"] = outcome_variables\n",
    "outcomes[\"descriptions\"] = variable_explanation\n",
    "outcomes[\"type\"] = variable_type\n",
    "outcomes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv with all patients and outcome variables\n",
    "\n",
    "data_outcome = raw_data_removed.copy()\n",
    "\n",
    "for col in raw_data_removed.columns:\n",
    "    if col not in outcome_variables:\n",
    "        data_outcome.drop(labels=col,axis=1,inplace=True)\n",
    "        \n",
    "data_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all missing values with NA\n",
    "\n",
    "data_outcome = data_outcome[data_outcome >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_outcome.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating into continous/categorical\n",
    "\n",
    "This needs to be done as to use hypothesis testing, different variable types need different tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_vars = [\"1 SGA\\n2 AGA\\n3 LGA\\n-77 missing birthweight customised centile comprised:\\nmiscarriage or termination <20w and not a case (n=15)\\nmiscarriage or termination <20w and spont PTB case but no birthweight customised centile (n=3)\\ntermination 20-22w or FDIU 20-22w with no birthweight customised centile (n=4)\",\n",
    "               \"1 Neg/trace \\n2 1+ or 0.3 g/L  \\n3 2+ or 1 g/L \\n4 3+ or >=3 g/L  \\n-67  No result among PET cases\\n-99 No result and case but not PET (n=525, 9.3%)\\n-909 Not a case (n=4580, 81.4%)\",\n",
    "               '1 \"Yes, pregnancy outcome known >=20w\"  \\n2 Pregnancy ended <20w  \\n3 \"Pregnancy ended <20w, but CASE\"  \\n4 Lost to follow up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_data_cont = data_outcome.copy()\n",
    "for col in data_outcome.columns:\n",
    "    if \"ontinuous\" not in outcomes[\"type\"][outcomes[\"variable\"].tolist().index(col)] and outcomes[\"type\"][outcomes[\"variable\"].tolist().index(col)] not in ordinal_vars:\n",
    "        outcome_data_cont.drop(labels=col,inplace=True,axis=1)\n",
    "        \n",
    "outcome_data_cont.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.cont_ord.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_data_bin = data_outcome.copy()\n",
    "for col in outcome_data_bin:\n",
    "    if col in outcome_data_cont:\n",
    "        outcome_data_bin.drop(columns=col,inplace=True)\n",
    "        \n",
    "outcome_data_bin.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.bin_cat.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reprepping for BioMarker Missingness\n",
    "\n",
    "After we started going through the biomarker data, we noticed there were risk_factor patients that were not present in the biomarker data. Thus, we had to remove them. We also have to do this for the PE labels and site labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select for only those patients in biomarker data\n",
    "\n",
    "# getting regids for all datasets as index\n",
    "data.index = raw_data_removed.index\n",
    "data_unscaled.index = raw_data_removed.index\n",
    "site_labels.index = raw_data_removed.index\n",
    "pe_labels.index = raw_data_removed.index\n",
    "data_outcome.index = raw_data_removed.index\n",
    "outcome_data_bin.index = raw_data_removed.index\n",
    "outcome_data_cont.index = raw_data_removed.index\n",
    "\n",
    "risk_data_removed = data.copy()\n",
    "risk_data_removed_unscaled = data_unscaled.copy()\n",
    "site_labels_removed = site_labels.copy()\n",
    "pe_labels_removed = pe_labels.copy()\n",
    "data_outcome_removed = data_outcome.copy()\n",
    "outcome_data_bin_removed = outcome_data_bin.copy()\n",
    "outcome_data_cont_removed = outcome_data_cont.copy()\n",
    "\n",
    "i = 0\n",
    "for p in data.index:\n",
    "    if p not in list(bio_data_removed[\"SCOPE Subject ID\"]):\n",
    "        risk_data_removed.drop(index=p,inplace=True)\n",
    "        risk_data_removed_unscaled.drop(index=p,inplace=True)\n",
    "        site_labels_removed.drop(index=p,inplace=True)\n",
    "        pe_labels_removed.drop(index=p,inplace=True)\n",
    "        data_outcome_removed.drop(index=p,inplace=True)\n",
    "        outcome_data_bin_removed.drop(index=p,inplace=True)\n",
    "        outcome_data_cont_removed.drop(index=p,inplace=True)\n",
    "        i += 1\n",
    "        \n",
    "risk_data_removed.to_csv(home_dir + \"GoodCopy/Data/risk_factors.cleaned.scaled.one_hot.biomarker.csv\")\n",
    "risk_data_removed_unscaled.to_csv(home_dir + \"GoodCopy/Data/risk_factors.cleaned.unscaled.one_hot.biomarker.csv\")\n",
    "site_labels_removed.to_csv(home_dir + \"GoodCopy/Data/site_labels.biomarker.csv\")\n",
    "pe_labels_removed.to_csv(home_dir + \"GoodCopy/Data/pe_labels.biomarker.csv\")\n",
    "data_outcome_removed.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.biomarker.csv\")\n",
    "outcome_data_bin_removed.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.bin_cat.biomarker.csv\")\n",
    "outcome_data_cont_removed.to_csv(home_dir + \"GoodCopy/Data/outcome_variables.unscaled.cont_ord.biomarker.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutting to only RFE selected variables\n",
    "\n",
    "Patricia did the RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE_summary\n",
    "\n",
    "rfe_sum = pd.read_csv(home_dir + \"GoodCopy/Data/rfe_summary_v2.csv\",index_col='varname').sort_values(\"votes\",ascending=False)\n",
    "rfe_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all variables which had 0 RFE votes. I only do this to scaled data, \n",
    "# as unscaled data is mainly used for hypothesis testing on cluster assingments\n",
    "\n",
    "risk_data_removed_drop = risk_data_removed.copy()\n",
    "\n",
    "for col in rfe_sum.index:\n",
    "    if rfe_sum.loc[col,\"votes\"] == 0:\n",
    "        risk_data_removed_drop.drop(columns = col,inplace=True)\n",
    "        \n",
    "risk_data_removed_drop.to_csv(home_dir + \"GoodCopy/Data/risk_factors.cleaned.scaled.one_hot.biomarker.RFE.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything into a dataset object and Creating a matched dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataSet object using all the things we did here.\n",
    "\n",
    "data = func.DataSet(input_data = risk_data_removed_drop.drop(columns=\"Unnamed: 0\"),\n",
    "                    input_data_unscaled = risk_data_removed_unscaled.drop(columns=\"Unnamed: 0\"), \n",
    "                    bio_data= scaled_bio_data,\n",
    "                    pe_labels = pe_labels_removed, site_labels = site_labels_removed, outcome_bin_cat = outcome_data_bin_removed, \n",
    "                    outcome_cont_ord = outcome_data_cont_removed,\n",
    "                    data_dict = data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.save_DataSet(home_dir + \"GoodCopy/Objects/data_object\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pappas_tadam",
   "language": "python",
   "name": "pappas_tadam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
