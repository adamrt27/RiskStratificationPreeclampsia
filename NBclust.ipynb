{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **6th** Notebook in the clustering pipeline. It shows you how to use a NBclust-like workflow to analyze clustering solutions. NBclust is an R package that evaluates clustering solutions on a handful of clustering metrics to determine the best one.\n",
    "\n",
    "<u>Note:</u> This is **NOT** a preferred way of evaluating clusters, as it evaluates things like how seperated the clusters are, not their biological relevance or robustness. I am working on a way to pick a solution that is better.\n",
    "\n",
    "Use <u>***NBclust_env***</u> virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to whatever directory GoodCopy is in\n",
    "\n",
    "home_dir = \"/home/l/lungboy/tadam/Project/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and functions\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pyckmeans.core.ckmeans as pyckmeans\n",
    "\n",
    "sys.path.append(home_dir + 'GoodCopy/Functions')\n",
    "\n",
    "import FunctionsOOPGood as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Datasets\n",
    "\n",
    "data = func.DataSet(empty=True)\n",
    "data.open_DataSet(home_dir + \"GoodCopy/Objects/data_object\")\n",
    "\n",
    "# data_matched = func.MatchedDataSet(empty=True)\n",
    "# data_matched.open_MatchedDataSet(home_dir + \"GoodCopy/Objects/matched_data_saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Labels\n",
    "\n",
    "agglo_labels = [pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_agglo_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "all_labels = [pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_all_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "hdb_labels = [pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_hdb_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "kmedoids_labels = [pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_kmedoids_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "snf_hdb_labels = [pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_snf_hdb_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "snf_spectral_labels =[pd.read_csv(home_dir + \"GoodCopy/EnsembleResults/{}_snf_spectral_labels.csv\".format(i), index_col= 0) for i in range(2,10)]\n",
    "\n",
    "labels = agglo_labels + all_labels + hdb_labels + kmedoids_labels + snf_hdb_labels + snf_spectral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remapping labels, so that they are numbered from 0-n, instead of some where they might be labeled [0,2,3,4] for example/\n",
    "# This strange numbering would be caused as the cluster ensemble algorithm would find less clusters than the max number.\n",
    "# This is needed as the following code relies on labels strucutre as [0,1,2,...,n]\n",
    "\n",
    "def remap_labels(lst):\n",
    "    # Create a dictionary to store the mapping of old labels to new labels\n",
    "    label_mapping = {label: new_label for new_label, label in enumerate(set(lst))}\n",
    "    \n",
    "    # Create a new list with updated labels\n",
    "    new_labels = [label_mapping[label] for label in lst]\n",
    "    \n",
    "    return new_labels\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = remap_labels(np.ravel(labels[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Adjusted Rand Index\n",
    "\n",
    "Adjusted Rand index to compare how well the clustering solution is representative of all generated solutions.\n",
    "\n",
    "Here I use the sklearn version of the function. Values closer to 1 are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "scores_ari = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for j in range(len(labels)):\n",
    "       \n",
    "        if i != j:\n",
    "            \n",
    "            res += adjusted_rand_score(labels[i],labels[j])\n",
    "    \n",
    "    res /= len(labels) - 1\n",
    "    \n",
    "    scores_ari.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "plt.plot(lol,scores_ari[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,scores_ari[8:16], label = \"All\")\n",
    "plt.plot(lol,scores_ari[16:24], label = \"HDB\")\n",
    "plt.plot(lol,scores_ari[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,scores_ari[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,scores_ari[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Normalized Mutual Information\n",
    "\n",
    "Normalized Mutual Information to compare how well the clustering solution is representative of all generated solutions.\n",
    "\n",
    "Here I use the sklearn version of the function. Values closer to 1 are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "scores_nmi = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    \n",
    "    res = 0\n",
    "    \n",
    "    for j in range(len(labels)):\n",
    "       \n",
    "        if i != j:\n",
    "            \n",
    "            res += normalized_mutual_info_score(labels[i],labels[j])\n",
    "    \n",
    "    res /= len(labels) - 1\n",
    "    \n",
    "    scores_nmi.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "plt.plot(lol,scores_nmi[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,scores_nmi[8:16], label = \"All\")\n",
    "plt.plot(lol,scores_nmi[16:24], label = \"HDB\")\n",
    "plt.plot(lol,scores_nmi[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,scores_nmi[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,scores_nmi[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Score\n",
    "Silhouette score from sklearn, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html.\n",
    "\n",
    "A value between -1 and 1 is given, where 1 is the best. It provides an indication of how well-separated the clusters are and how similar data points are within their own cluster compared to other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    sil.append(silhouette_score(data.gower, np.ravel(labels[i]),metric=\"precomputed\"))\n",
    "    \n",
    "lol = [2,3,4,5,6,7,8,9]\n",
    "    \n",
    "plt.plot(lol,sil[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,sil[8:16], label = \"All\")\n",
    "plt.plot(lol,sil[16:24], label = \"HDB\")\n",
    "plt.plot(lol,sil[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,sil[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,sil[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Info Criterion\n",
    "Bayesion info criterion for clustering, taken from pyckmeans package, https://pyckmeans.readthedocs.io/en/latest/pyckmeans.core.html.\n",
    "\n",
    "A higher value is better. In the context of clustering, BIC assesses the trade-off between the goodness of fit of the model (how well the data fits the clusters) and the complexity of the model (the number of parameters required to define the clusters). It penalizes models with more parameters, discouraging overfitting.\n",
    "\n",
    "*Note: first plot, using the data in the bic array allows the bic algorithm to compute centroids, while the second plot, using the data in the bic_wmed array allows the bic algorithm to use precomputed medoids. The medoids are computed using the function I wrote in FunctionsOOP, particularily the DataSet.compute_medoids_from_distances() function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyckmeans.core.ckmeans as pyckmeans\n",
    "\n",
    "bic = []\n",
    "bic_wmed = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    bic.append(pyckmeans.bic_kmeans(np.asarray(data.input_data),np.ravel(labels[i])))\n",
    "    bic_wmed.append(pyckmeans.bic_kmeans(np.asarray(data.input_data),np.ravel(labels[i]), \n",
    "                                         np.asarray(data.compute_medoids_from_distances(np.ravel(labels[i])))))\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "lol = [2,3,4,5,6,7,8,9]\n",
    "\n",
    "plt.plot(lol,bic[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,bic[8:16], label = \"All\")\n",
    "plt.plot(lol,bic[16:24], label = \"HDB\")\n",
    "plt.plot(lol,bic[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,bic[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,bic[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lol,bic_wmed[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,bic_wmed[8:16], label = \"All\")\n",
    "plt.plot(lol,bic_wmed[16:24], label = \"HDB\")\n",
    "plt.plot(lol,bic_wmed[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,bic_wmed[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,bic_wmed[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gap Statistic\n",
    "\n",
    "Gap statistic method adjusted from various internet sources + chatGPT!\n",
    "\n",
    "A higher gap statistic is better. The gap statistic compares the goodness of fit of the clustering solution obtained from the data with the goodness of fit that would be expected from a null reference distribution. It essentially can determine if a clustering solution is statistically significant.\n",
    "\n",
    "Ouputs of the method:\n",
    "\n",
    "* gap_value: Gap Statistic. The overall metric returned by this algorithm.\n",
    "* sdk: Standard Deviation of log dispersion. It provides information about the spread of log dispersions and helps in assessing the reliability of the gap statistic.\n",
    "* sk: Standard Error of Gap Statistic. It helps quantify the uncertainty associated with the gap statistic and is used for error estimation.\n",
    "* gap_star: Gap* Statistic. Uses non logarithmic values, so better for smaller datasets.\n",
    "* sdk_star: Standard Deviation of log dispersion. Same as sdk but for Gap*.\n",
    "* sk_star: Standard Error of Gap Statistic. Same as sk but for Gap*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to compute gap statistic\n",
    "\n",
    "def _calculate_dispersion(X = pd.DataFrame, labels = np.ndarray, centroids = np.ndarray):\n",
    "        \"\"\"\n",
    "        Calculate the dispersion between actual points and their assigned centroids\n",
    "        \"\"\"\n",
    "        disp = np.sum(\n",
    "            np.sum(\n",
    "                [np.abs(inst - centroids[label]) ** 2 for inst, label in zip(X, labels)]\n",
    "            )\n",
    "        )\n",
    "        return disp\n",
    "    \n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def _calculate_gap(X: pd.DataFrame, n_refs: int, n_clusters: int, labels, medoids):\n",
    "    \"\"\"\n",
    "    Calculate the gap value of the given data, n_refs, and number of clusters.\n",
    "    Return the resutling gap value and n_clusters\n",
    "    \"\"\"\n",
    "    # Holder for reference dispersion results\n",
    "    ref_dispersions = np.zeros(n_refs)\n",
    "\n",
    "    # Compute the range of each feature\n",
    "    X = np.asarray(X)\n",
    "    a, b = X.min(axis=0, keepdims=True), X.max(axis=0, keepdims=True)\n",
    "\n",
    "    # For n_references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "    for i in range(n_refs):\n",
    "        # Create new random reference set uniformly over the range of each feature\n",
    "        random_data = np.random.random_sample(size=X.shape) * (b - a) + a\n",
    "\n",
    "        # Fit to it, getting the centroids and labels, and add to accumulated reference dispersions array.\n",
    "        centroids, labels = KMedoids(n_clusters).fit(random_data).cluster_centers_, KMedoids(n_clusters).fit(random_data).labels_  # type: Tuple[np.ndarray, np.ndarray]\n",
    "        dispersion = _calculate_dispersion(\n",
    "            X=random_data, labels=labels, centroids=centroids\n",
    "        )\n",
    "        ref_dispersions[i] = dispersion\n",
    "\n",
    "    # Fit cluster to original data and create dispersion calc.\n",
    "    dispersion = _calculate_dispersion(X=X, labels=labels, centroids=medoids)\n",
    "\n",
    "    # Calculate gap statistic\n",
    "    ref_log_dispersion = np.mean(np.log(ref_dispersions))\n",
    "    log_dispersion = np.log(dispersion)\n",
    "    gap_value = ref_log_dispersion - log_dispersion\n",
    "    # compute standard deviation\n",
    "    sdk = np.sqrt(np.mean((np.log(ref_dispersions) - ref_log_dispersion) ** 2.0))\n",
    "    sk = np.sqrt(1.0 + 1.0 / n_refs) * sdk\n",
    "\n",
    "    # Calculate Gap* statistic\n",
    "    # by \"A comparison of Gap statistic definitions with and\n",
    "    # with-out logarithm function\"\n",
    "    # https://core.ac.uk/download/pdf/12172514.pdf\n",
    "    gap_star = np.mean(ref_dispersions) - dispersion\n",
    "    sdk_star = np.sqrt(np.mean((ref_dispersions - dispersion) ** 2.0))\n",
    "    sk_star = np.sqrt(1.0 + 1.0 / n_refs) * sdk_star\n",
    "\n",
    "    return gap_value, sdk, sk, gap_star, sdk_star, sk_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually computing the gap statistic. Will probably take 10-20 minutes to run, depending on your n_refs. \n",
    "# An ideal n_refs value is above 100, but the higher n_refs, the longer the algorithm will take (O(n) complexity).\n",
    "\n",
    "gaps = []\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(2,10):\n",
    "        \n",
    "        ind = int(j-2 + (i*8))\n",
    "        #print(ind)\n",
    "        gaps.append(_calculate_gap(X = np.asarray(data.input_data.copy()), n_refs = 100, \n",
    "                                   n_clusters = len(np.unique(labels[ind])),\n",
    "                                   labels = np.ravel(labels[ind]),\n",
    "                                   medoids = np.asarray(data.compute_medoids_from_distances(np.ravel(labels[ind])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[:8]], yerr = [g[1] for g in gaps[:8]], capsize=5, fmt=\"-o\", label=\"Agglomerative\")\n",
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[8:16]], yerr = [g[1] for g in gaps[8:16]], capsize=5, fmt=\"-o\", label = \"All\")\n",
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[16:24]], yerr = [g[1] for g in gaps[16:24]], capsize=5, fmt=\"-o\", label = \"HDB\")\n",
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[24:32]], yerr = [g[1] for g in gaps[24:32]], capsize=5, fmt=\"-o\", label  = \"Kmedoid\")\n",
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[32:40]], yerr = [g[1] for g in gaps[32:40]], capsize=5, fmt=\"-o\", label = \"SNF HDB\")\n",
    "plt.errorbar([2,3,4,5,6,7,8,9],[g[0] for g in gaps[40:48]], yerr = [g[1] for g in gaps[40:48]], capsize=5, fmt=\"-o\", label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow Method\n",
    "\n",
    "Uses the above *\\_calculate_dispersion()* function to compute the intracluster dispersion, which is how spread out points in a cluster are from their medoid. A lower value is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elb = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "\n",
    "    elb.append(_calculate_dispersion(np.asarray(data.input_data),np.asarray(labels[i]),np.asarray(data.compute_medoids_from_distances(labels[i]))))\n",
    "    \n",
    "plt.plot(lol,elb[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,elb[8:16], label = \"All\")\n",
    "plt.plot(lol,elb[16:24], label = \"HDB\")\n",
    "plt.plot(lol,elb[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,elb[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,elb[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calinski Harabasz\n",
    "\n",
    "Calinski Harabasz method from scikit learn, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html.\n",
    "\n",
    "A higher value is better. The score is defined as ratio of the sum of between-cluster dispersion and of within-cluster dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "cal = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    cal.append(calinski_harabasz_score(data.input_data,labels[i]))\n",
    "    \n",
    "plt.plot(lol,cal[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,cal[8:16], label = \"All\")\n",
    "plt.plot(lol,cal[16:24], label = \"HDB\")\n",
    "plt.plot(lol,cal[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,cal[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,cal[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Davies Bouldin\n",
    "\n",
    "Davies Bouldin method from scikit learn, https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html.\n",
    "\n",
    "Lower values is better. The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "dav = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    dav.append(davies_bouldin_score(data.input_data,labels[i]))\n",
    "    \n",
    "plt.plot(lol,dav[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,dav[8:16], label = \"All\")\n",
    "plt.plot(lol,dav[16:24], label = \"HDB\")\n",
    "plt.plot(lol,dav[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,dav[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,dav[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dunn Index\n",
    "\n",
    "Dunn index, adapted from internet sources + chatGPT.\n",
    "\n",
    "A higher solution is better. It measures the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. The Dunn Index aims to capture both the compactness of clusters and the separation between clusters, making it a useful criterion for assessing the clustering structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(distances, labels):\n",
    "    \"\"\"\n",
    "    Calculate the Dunn index for a given clustering result.\n",
    "\n",
    "    Parameters:\n",
    "    distances (numpy.ndarray): n x n distance matrix, where n is the number of data points.\n",
    "    labels (numpy.ndarray): Cluster assignments for each data point.\n",
    "\n",
    "    Returns:\n",
    "    float: Dunn index value.\n",
    "    \"\"\"\n",
    "    # Number of clusters (unique labels)\n",
    "    num_clusters = len(np.unique(labels))\n",
    "\n",
    "    # Calculate the cluster diameters (maximum distance within each cluster)\n",
    "    cluster_diameters = np.zeros(num_clusters)\n",
    "    for label in range(num_clusters):\n",
    "        cluster_data = distances[labels == label][:, labels == label]\n",
    "        cluster_diameters[label] = np.max(cluster_data)\n",
    "\n",
    "    # Calculate the inter-cluster distance (minimum distance between clusters)\n",
    "    inter_cluster_distance = np.min(distances[distances > 0])\n",
    "\n",
    "    # Calculate the Dunn index\n",
    "    dunn_index_value = inter_cluster_distance / np.max(cluster_diameters)\n",
    "\n",
    "    return dunn_index_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dunn = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    dunn.append(dunn_index(np.asarray(data.gower),np.asarray(labels[i])))\n",
    "    \n",
    "plt.plot(lol,dunn[:8], label = \"Agglomerative\")\n",
    "plt.plot(lol,dunn[8:16], label = \"All\")\n",
    "plt.plot(lol,dunn[16:24], label = \"HDB\")\n",
    "plt.plot(lol,dunn[24:32], label = \"Kmedoid\")\n",
    "plt.plot(lol,dunn[32:40], label = \"SNF HDB\")\n",
    "plt.plot(lol,dunn[40:48], label = \"SNF Spectral\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this section we add up all the scores for each solution to find the best overall solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking based scoring\n",
    "Here we score based on ranking when compared to other solutions and then sum up ranks to find best overall solution.\n",
    "\n",
    "**Lower score is better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df = pd.DataFrame(np.vstack((sil,bic,bic_wmed,dunn,dav, cal, elb,[g[0] for g in gaps], scores_ari, scores_nmi)).T,\n",
    "                   index=[f'{method} {i}' for method in [\"Agglomerative\", \"All\", \"HDB\", \"Kmedoid\", \"SNF HDB\", \"SNF Spectral\"] for i in range(2,10)],\n",
    "                columns = [\"Silhouette\",\"BIC Centroid\",\"BIC Medoid\",\"Dunn\",\"Davies Bouldin\",\"Calinski Harabazc\",\"Elbow\",\"Gap\", \"Adjusted Rand Index\", \"Normalized Mutual Information\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing a score based on ranking\n",
    "\n",
    "sum_df.rank().pow({\"Silhouette\":-1,\"BIC Centroid\":-1,\"BIC Medoid\":-1,\"Davies Bouldin\":1,\"Dunn\":-1,\"Calinski Harabazc\":-1,\"Elbow\":1,\"Gap\":-1, \"Adjusted Rand Index\":-1,\"Normalized Mutual Information\":-1}).mul({\"Silhouette\":1,\"BIC Centroid\":1,\"BIC Medoid\":1,\"Davies Bouldin\":0.25,\"Dunn\":1,\"Calinski Harabazc\":1,\"Elbow\":1,\"Gap\":1, \"Adjusted Rand Index\":1,\"Normalized Mutual Information\":1}).sum(axis=1).sort_values(ascending=True)\n",
    "\n",
    "# Smallest score is best solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled-based scoring\n",
    "Here we score based on scaled test statistics and then sum up scaled scores and rank compared to other solutions. \n",
    "\n",
    "**Lower score is better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom scaling range (1 to 10)\n",
    "custom_min = 1\n",
    "custom_max = 10\n",
    "\n",
    "df = sum_df.copy()\n",
    "\n",
    "# Apply Min-Max scaling with custom range\n",
    "scaled_data = (df - df.min()) / (df.max() - df.min()) * (custom_max - custom_min) + custom_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.rank().pow({\"Silhouette\":-1,\"BIC Centroid\":-1,\"BIC Medoid\":-1,\"Davies Bouldin\":1,\"Dunn\":-1,\"Calinski Harabazc\":-1,\"Elbow\":1,\"Gap\":-1, \"Adjusted Rand Index\":-1,\"Normalized Mutual Information\":-1}).mul({\"Silhouette\":1,\"BIC Centroid\":1,\"BIC Medoid\":1,\"Davies Bouldin\":0.25,\"Dunn\":1,\"Calinski Harabazc\":1,\"Elbow\":1,\"Gap\":1, \"Adjusted Rand Index\":1,\"Normalized Mutual Information\":1}).sum(axis=1).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBclust_env",
   "language": "python",
   "name": "nbclust_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
